"""
FastAPI - Microservicio de Predicci√≥n SLA
"""
import logging
from datetime import datetime
from typing import List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import asyncio
from concurrent.futures import ThreadPoolExecutor

from .config import get_settings
from .schemas import (
    PrediccionRequest,
    PrediccionResponse,
    PrediccionBatchResponse,
    ResumenPrediccion,
    TendenciaItem,
    HealthResponse,
    ReentrenamientoResponse
)
from .database import (
    get_solicitudes_activas,
    get_tendencias_historicas,
    verificar_conexion,
    get_filtros_disponibles
)
from .model import (
    get_modelo,
    predecir,
    predecir_batch,
    forzar_reentrenamiento,
    modelo_esta_cargado,
    get_modelo_info
)

# Configuraci√≥n
settings = get_settings()

# Logging
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Thread pool para operaciones pesadas
executor = ThreadPoolExecutor(max_workers=4)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gesti√≥n del ciclo de vida de la aplicaci√≥n"""
    # Startup
    logger.info("üöÄ Iniciando servicio de predicci√≥n SLA...")
    
    # Pre-cargar modelo
    try:
        await asyncio.get_event_loop().run_in_executor(executor, get_modelo)
        logger.info("‚úÖ Modelo cargado exitosamente")
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Error al cargar modelo: {e}")
    
    yield
    
    # Shutdown
    logger.info("üëã Cerrando servicio de predicci√≥n...")
    executor.shutdown(wait=True)


# Crear aplicaci√≥n FastAPI
app = FastAPI(
    title="SLA Predicci√≥n Service",
    description="Microservicio de Machine Learning para predicci√≥n de incumplimientos SLA",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especifica tus dominios
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =============================================================================
# ENDPOINTS
# =============================================================================

@app.get("/", tags=["Info"])
async def root():
    """Informaci√≥n del servicio"""
    return {
        "service": "SLA Predicci√≥n Service",
        "version": "1.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "docs": "/docs"
    }


@app.get("/health", response_model=HealthResponse, tags=["Info"])
async def health_check():
    """
    Health check para Docker/Kubernetes.
    Verifica que el servicio y el modelo est√©n operativos.
    """
    return HealthResponse(
        status="healthy" if modelo_esta_cargado() else "degraded",
        model_loaded=modelo_esta_cargado(),
        timestamp=datetime.now(),
        version="1.0.0"
    )


@app.get("/filtros", tags=["Info"])
async def obtener_filtros():
    """
    Obtiene las opciones de filtros disponibles desde la BD.
    
    Retorna c√≥digos SLA activos, roles y bloques tecnol√≥gicos.
    √ötil para cargar din√°micamente las opciones de filtrado en el frontend.
    """
    try:
        filtros = await asyncio.get_event_loop().run_in_executor(
            executor,
            get_filtros_disponibles
        )
        return filtros
    except Exception as e:
        logger.error(f"Error al obtener filtros: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/predecir", response_model=PrediccionResponse, tags=["Predicci√≥n"])
async def predecir_individual(request: PrediccionRequest):
    """
    Predicci√≥n individual para una solicitud espec√≠fica.
    
    Uso: Cuando el usuario ve el detalle de una solicitud.
    Tiempo de respuesta esperado: < 50ms
    """
    try:
        probabilidad, nivel_riesgo, factores = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: predecir(
                request.dias_transcurridos,
                request.dias_umbral,
                request.id_rol
            )
        )
        
        return PrediccionResponse(
            id_solicitud=request.id_solicitud,
            probabilidad_incumplimiento=round(probabilidad, 4),
            nivel_riesgo=nivel_riesgo,
            fecha_prediccion=datetime.now(),
            factores_riesgo=factores
        )
    except Exception as e:
        logger.error(f"Error en predicci√≥n individual: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/predecir/criticas", response_model=List[PrediccionResponse], tags=["Predicci√≥n"])
async def predecir_criticas(
    limite: int = Query(default=20, ge=1, le=100, description="M√°ximo de resultados")
):
    """
    Predicci√≥n de solicitudes cr√≠ticas (pr√≥ximas a vencer).
    
    Optimizado para dashboard. Solo analiza solicitudes con 70%+ del tiempo consumido.
    Tiempo de respuesta esperado: < 200ms
    """
    try:
        # Obtener solicitudes cr√≠ticas
        solicitudes = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: get_solicitudes_activas(solo_criticas=True, limite=limite)
        )
        
        if not solicitudes:
            return []
        
        # Predicci√≥n batch
        resultados = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: predecir_batch(solicitudes)
        )
        
        # Mapear a response y ordenar por probabilidad
        predicciones = [
            PrediccionResponse(
                id_solicitud=r['id_solicitud'],
                codigo_sla=r['codigo_sla'],
                nombre_rol=r['nombre_rol'],
                probabilidad_incumplimiento=r['probabilidad_incumplimiento'],
                nivel_riesgo=r['nivel_riesgo'],
                dias_restantes=r['dias_restantes'],
                fecha_prediccion=datetime.now(),
                factores_riesgo=r['factores_riesgo']
            )
            for r in resultados
        ]
        
        # Ordenar por probabilidad descendente
        predicciones.sort(key=lambda x: x.probabilidad_incumplimiento, reverse=True)
        
        return predicciones
        
    except Exception as e:
        logger.error(f"Error en predicciones cr√≠ticas: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/predecir/paginado", response_model=PrediccionBatchResponse, tags=["Predicci√≥n"])
async def predecir_paginado(
    pagina: int = Query(default=1, ge=1, description="N√∫mero de p√°gina"),
    tamano_pagina: int = Query(default=50, ge=1, le=100, description="Registros por p√°gina"),
    incluir_historicas: bool = Query(default=True, description="Incluir solicitudes completadas/canceladas"),
    codigo_sla: str = Query(default=None, description="Filtrar por c√≥digo SLA (ej: SLA1, SLA2)")
):
    """
    Predicci√≥n paginada para tabla completa.
    
    Optimizado para grandes vol√∫menes. No carga todo a memoria.
    Por defecto incluye todas las solicitudes (activas e hist√≥ricas).
    Tiempo de respuesta esperado: < 500ms
    """
    try:
        # Obtener solicitudes paginadas
        solicitudes, total = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: get_solicitudes_activas(
                pagina=pagina,
                tamano_pagina=tamano_pagina,
                con_total=True,
                incluir_historicas=incluir_historicas,
                codigo_sla=codigo_sla
            )
        )
        
        if not solicitudes:
            return PrediccionBatchResponse(
                data=[],
                pagina=pagina,
                tamano_pagina=tamano_pagina,
                total_registros=total,
                total_paginas=0
            )
        
        # Predicci√≥n batch
        resultados = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: predecir_batch(solicitudes)
        )
        
        # Mapear a response
        predicciones = [
            PrediccionResponse(
                id_solicitud=r['id_solicitud'],
                codigo_sla=r['codigo_sla'],
                nombre_rol=r['nombre_rol'],
                probabilidad_incumplimiento=r['probabilidad_incumplimiento'],
                nivel_riesgo=r['nivel_riesgo'],
                dias_restantes=r['dias_restantes'],
                fecha_prediccion=datetime.now(),
                factores_riesgo=r['factores_riesgo']
            )
            for r in resultados
        ]
        
        total_paginas = (total + tamano_pagina - 1) // tamano_pagina if total > 0 else 0
        
        return PrediccionBatchResponse(
            data=predicciones,
            pagina=pagina,
            tamano_pagina=tamano_pagina,
            total_registros=total,
            total_paginas=total_paginas
        )
        
    except Exception as e:
        logger.error(f"Error en predicci√≥n paginada: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/resumen", response_model=ResumenPrediccion, tags=["Dashboard"])
async def obtener_resumen():
    """
    Resumen r√°pido para KPIs del dashboard.
    
    Retorna conteos por nivel de riesgo y promedio general.
    Tiempo de respuesta esperado: < 300ms
    """
    try:
        # Obtener predicciones cr√≠ticas para calcular resumen
        solicitudes = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: get_solicitudes_activas(solo_criticas=True, limite=100)
        )
        
        if not solicitudes:
            return ResumenPrediccion(
                total_analizadas=0,
                criticas=0,
                altas=0,
                medias=0,
                bajas=0,
                promedio_riesgo=0
            )
        
        # Predicci√≥n batch
        resultados = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: predecir_batch(solicitudes)
        )
        
        # Contar por nivel
        niveles = {"CRITICO": 0, "ALTO": 0, "MEDIO": 0, "BAJO": 0}
        suma_prob = 0
        
        for r in resultados:
            niveles[r['nivel_riesgo']] += 1
            suma_prob += r['probabilidad_incumplimiento']
        
        promedio = (suma_prob / len(resultados) * 100) if resultados else 0
        
        return ResumenPrediccion(
            total_analizadas=len(resultados),
            criticas=niveles["CRITICO"],
            altas=niveles["ALTO"],
            medias=niveles["MEDIO"],
            bajas=niveles["BAJO"],
            promedio_riesgo=round(promedio, 1)
        )
        
    except Exception as e:
        logger.error(f"Error al obtener resumen: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/tendencias", response_model=List[TendenciaItem], tags=["Dashboard"])
async def obtener_tendencias(
    meses: int = Query(default=6, ge=1, le=24, description="Meses hacia atr√°s")
):
    """
    Tendencias hist√≥ricas de cumplimiento SLA.
    
    √ötil para gr√°ficos de evoluci√≥n temporal.
    """
    try:
        tendencias = await asyncio.get_event_loop().run_in_executor(
            executor,
            lambda: get_tendencias_historicas(meses)
        )
        
        return [
            TendenciaItem(
                periodo=t['periodo'],
                total_solicitudes=t['total_solicitudes'],
                incumplidas=t['incumplidas'],
                tasa_incumplimiento=float(t['tasa_incumplimiento'] or 0)
            )
            for t in tendencias
        ]
        
    except Exception as e:
        logger.error(f"Error al obtener tendencias: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/modelo/reentrenar", response_model=ReentrenamientoResponse, tags=["Admin"])
async def reentrenar_modelo():
    """
    Fuerza el reentrenamiento del modelo con datos actuales.
    
    Uso: Llamar peri√≥dicamente o cuando hay muchos datos nuevos.
    Este endpoint puede tardar varios segundos.
    """
    try:
        resultado = await asyncio.get_event_loop().run_in_executor(
            executor,
            forzar_reentrenamiento
        )
        
        return ReentrenamientoResponse(
            status="ok",
            message="Modelo reentrenado exitosamente",
            samples_used=resultado['samples_used'],
            accuracy=resultado['accuracy'],
            timestamp=resultado['timestamp']
        )
        
    except Exception as e:
        logger.error(f"Error al reentrenar modelo: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/modelo/info", tags=["Admin"])
async def info_modelo():
    """Obtiene informaci√≥n del modelo actual"""
    return get_modelo_info()


# =============================================================================
# ERROR HANDLERS
# =============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Error no manejado: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Error interno del servidor", "error": str(exc)}
    )


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=True
    )
